{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed1ef8aa-8824-487a-a44e-921ba4db6ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import rioxarray as rioxr\n",
    "\n",
    "import geopandas as geopd\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fca90a-e708-4fd1-a86c-42fcc54924ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script processes and concatenates snow water equivalent (SWE) data for catchments.\n",
    "# It reads SWE data from NetCDF files, clips the data to the catchment geometries, and calculates the mean SWE for each catchment.\n",
    "# The results are saved as a CSV file.\n",
    "\n",
    "# Path to the vector watershed file\n",
    "vector_watershed_path = \"/path/to/CAMELS-FI_catchments.gpkg\"\n",
    "watersheds = geopd.read_file(vector_watershed_path, layer='v1')\n",
    "\n",
    "# Reprojecting to WGS84 so there's no need to reproject within loop\n",
    "watersheds = watersheds.to_crs(epsg=4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abaffc47-04c4-417f-8d62-37f9970f5b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swe_catchment_mean(args):\n",
    "    file, watersheds = args\n",
    "    swe = pd.DataFrame(columns=watersheds.Paikka_Id, index=pd.to_datetime([]))\n",
    "    swe.index.name = 'date'\n",
    "    \n",
    "    with xr.open_dataset(file, mask_and_scale=True, decode_coords='all') as dataset:\n",
    "        data_array = dataset['swe']\n",
    "        time = pd.to_datetime(data_array.time.item())\n",
    "        data_array = data_array.loc[:, 59:71, 20:32]\n",
    "        data_array = xr.where(data_array < 0, np.nan, data_array)\n",
    "        data_array = data_array.interpolate_na('lon', limit=3, max_gap=1)\n",
    "        data_array = data_array.interpolate_na('lat', limit=2, max_gap=1)\n",
    "        data_array = data_array.rio.write_crs(\"epsg:4326\")\n",
    "        data_array = data_array.rename(lon='longitude', lat='latitude')\n",
    "        \n",
    "        swe_list = []\n",
    "        for i in range(len(watersheds)):\n",
    "            watershed = watersheds.loc[[i]]\n",
    "            mean_swe = data_array.rio.clip(watershed.geometry.values, crs=watershed.crs, all_touched=True).mean().item()\n",
    "            swe_list.append(round(mean_swe, 2))\n",
    "            \n",
    "    swe.loc[time] = swe_list\n",
    "    return swe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c0108a0-e9f1-4fb3-aae1-940c327e4202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory containing SWE data NetCDF files\n",
    "root = \"/path/to/snow/data/swe/MERGED/v3.1\"\n",
    "glob_root = root + '/**/**/*.nc'\n",
    "files = {file for file in glob.iglob(glob_root, recursive=True)}\n",
    "files = list(files)\n",
    "assert len(files) > 0, f\"No files found with glob {glob_root}, check path\"\n",
    "\n",
    "# Avoiding opening the same file multiple times by doing all the same operations with the same file open\n",
    "arg_list = []\n",
    "for file in files:\n",
    "    counter += 1\n",
    "    arg_list.append((file, watersheds))\n",
    "\n",
    "# Because of a memory leak, these need to be divided into chunks\n",
    "chunks = []\n",
    "chunk_size = 108\n",
    "for i in range(0, len(arg_list), chunk_size):\n",
    "    chunks.append(arg_list[i:i + chunk_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917dc73c-fbd5-4c52-ad33-c6288b9fe982",
   "metadata": {},
   "outputs": [],
   "source": [
    "swe_list = []\n",
    "for chunk in tqdm(chunks):\n",
    "    with Pool(18) as p:\n",
    "        swe_list.append(p.map(swe_catchment_mean, chunk))\n",
    "\n",
    "swe_list_flat = []\n",
    "for chunk in swe_list:\n",
    "    swe_list_flat.extend(chunk)\n",
    "\n",
    "swe = pd.concat(swe_list_flat)\n",
    "swe = swe.sort_index()\n",
    "swe.to_csv(\"/path/to/timeseries_by_attribute/swe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb1fc0f-a0df-488d-950e-3c94873fac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the SWE data table\n",
    "swe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
